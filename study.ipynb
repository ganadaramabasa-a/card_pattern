{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36c4ed8d",
   "metadata": {},
   "source": [
    "# 신용카드 사기 탐지 모델 성능 비교\n",
    "\n",
    "성능을 비교하여 신용카드 사기 탐지에 가장 적합한 모델 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e679815d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                             roc_auc_score, precision_recall_curve, \n",
    "                             average_precision_score, f1_score, roc_curve)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch 임포트 및 GPU 설정\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"GPU 설정 확인\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"PyTorch 버전: {torch.__version__}\")\n",
    "print(f\"CUDA 사용 가능: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA 버전: {torch.version.cuda}\")\n",
    "    print(f\"cuDNN 버전: {torch.backends.cudnn.version()}\")\n",
    "    print(f\"GPU 개수: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    device = torch.device('cuda:0')\n",
    "    print(f\"\\nGPU 사용 설정 완료: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"GPU를 찾을 수 없음. CPU 사용\")\n",
    "\n",
    "# 한글 폰트 설정\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(f\"\\n사용할 디바이스: {device}\")\n",
    "print(\"\\n라이브러리 로드 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c122635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "data_path = r\"creditcard.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"데이터 shape: {df.shape}\")\n",
    "print(f\"\\n컬럼 목록: {df.columns.tolist()}\")\n",
    "print(f\"\\n데이터 타입:\")\n",
    "print(df.dtypes)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a746d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 통계 및 클래스 분포 확인\n",
    "print(\"=\" * 50)\n",
    "print(\"기본 통계 정보\")\n",
    "print(\"=\" * 50)\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"클래스 분포 (Class: 0=정상, 1=사기)\")\n",
    "print(\"=\" * 50)\n",
    "class_counts = df['Class'].value_counts()\n",
    "print(class_counts)\n",
    "print(f\"\\n사기 비율: {class_counts[1] / len(df) * 100:.4f}%\")\n",
    "\n",
    "# 클래스 분포 시각화\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].bar(['정상 (0)', '사기 (1)'], class_counts.values, color=['steelblue', 'crimson'])\n",
    "axes[0].set_title('클래스 분포')\n",
    "axes[0].set_ylabel('건수')\n",
    "for i, v in enumerate(class_counts.values):\n",
    "    axes[0].text(i, v + 1000, f'{v:,}', ha='center')\n",
    "\n",
    "axes[1].pie(class_counts.values, labels=['정상', '사기'], autopct='%1.3f%%', \n",
    "            colors=['steelblue', 'crimson'], explode=[0, 0.1])\n",
    "axes[1].set_title('클래스 비율')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12237ea4",
   "metadata": {},
   "source": [
    "## 2. 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99b80f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리\n",
    "# 결측치 확인\n",
    "print(\"결측치 개수:\")\n",
    "print(df.isnull().sum().sum())\n",
    "\n",
    "# 특성과 타겟 분리\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "# Time과 Amount 스케일링 (V1~V28은 이미 PCA 변환되어 있음)\n",
    "scaler = StandardScaler()\n",
    "X['Time'] = scaler.fit_transform(X[['Time']])\n",
    "X['Amount'] = scaler.fit_transform(X[['Amount']])\n",
    "\n",
    "# 학습/테스트 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\n학습 데이터: {X_train.shape}\")\n",
    "print(f\"테스트 데이터: {X_test.shape}\")\n",
    "print(f\"\\n학습 데이터 클래스 분포:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\n테스트 데이터 클래스 분포:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc82eba",
   "metadata": {},
   "source": [
    "## 3. 전통적인 분류 모델\n",
    "\n",
    "### 3.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31926aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 저장용 딕셔너리\n",
    "results = {}\n",
    "\n",
    "# Logistic Regression\n",
    "print(\"=\" * 50)\n",
    "print(\"Logistic Regression 학습 중...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# 예측\n",
    "lr_pred = lr_model.predict(X_test)\n",
    "lr_prob = lr_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 성능 평가\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, lr_pred, target_names=['정상', '사기']))\n",
    "\n",
    "lr_auc = roc_auc_score(y_test, lr_prob)\n",
    "lr_f1 = f1_score(y_test, lr_pred)\n",
    "lr_ap = average_precision_score(y_test, lr_prob)\n",
    "\n",
    "print(f\"ROC-AUC Score: {lr_auc:.4f}\")\n",
    "print(f\"F1 Score: {lr_f1:.4f}\")\n",
    "print(f\"Average Precision: {lr_ap:.4f}\")\n",
    "\n",
    "results['Logistic Regression'] = {\n",
    "    'predictions': lr_pred,\n",
    "    'probabilities': lr_prob,\n",
    "    'roc_auc': lr_auc,\n",
    "    'f1': lr_f1,\n",
    "    'avg_precision': lr_ap\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599d704f",
   "metadata": {},
   "source": [
    "### 3.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daa378d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "print(\"=\" * 50)\n",
    "print(\"Random Forest 학습 중...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, \n",
    "                                   class_weight='balanced', n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# 예측\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "rf_prob = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 성능 평가\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, rf_pred, target_names=['정상', '사기']))\n",
    "\n",
    "rf_auc = roc_auc_score(y_test, rf_prob)\n",
    "rf_f1 = f1_score(y_test, rf_pred)\n",
    "rf_ap = average_precision_score(y_test, rf_prob)\n",
    "\n",
    "print(f\"ROC-AUC Score: {rf_auc:.4f}\")\n",
    "print(f\"F1 Score: {rf_f1:.4f}\")\n",
    "print(f\"Average Precision: {rf_ap:.4f}\")\n",
    "\n",
    "results['Random Forest'] = {\n",
    "    'predictions': rf_pred,\n",
    "    'probabilities': rf_prob,\n",
    "    'roc_auc': rf_auc,\n",
    "    'f1': rf_f1,\n",
    "    'avg_precision': rf_ap\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7582e1",
   "metadata": {},
   "source": [
    "## 4. 이상치 탐지 특화 모델\n",
    "\n",
    "### 4.1 One-Class SVM\n",
    "정상 데이터만으로 학습하여 이상치를 탐지하는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729a76ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Class SVM (정상 데이터만으로 학습)\n",
    "print(\"=\" * 50)\n",
    "print(\"One-Class SVM 학습 중...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 정상 데이터만 추출하여 학습 (샘플링하여 계산 시간 단축)\n",
    "X_train_normal = X_train[y_train == 0]\n",
    "# 계산 효율을 위해 정상 데이터 일부만 샘플링\n",
    "sample_size = min(10000, len(X_train_normal))\n",
    "X_train_sample = X_train_normal.sample(n=sample_size, random_state=42)\n",
    "\n",
    "print(f\"학습에 사용할 정상 데이터 샘플 수: {len(X_train_sample)}\")\n",
    "\n",
    "# One-Class SVM 학습\n",
    "ocsvm_model = OneClassSVM(kernel='rbf', gamma='scale', nu=0.01)\n",
    "ocsvm_model.fit(X_train_sample)\n",
    "\n",
    "# 예측 (1: 정상, -1: 이상치)\n",
    "ocsvm_pred_raw = ocsvm_model.predict(X_test)\n",
    "# 이상치(-1)를 사기(1)로, 정상(1)을 정상(0)으로 변환\n",
    "ocsvm_pred = np.where(ocsvm_pred_raw == -1, 1, 0)\n",
    "\n",
    "# decision_function으로 확률 대용 점수 계산 (낮을수록 이상치)\n",
    "ocsvm_scores = -ocsvm_model.decision_function(X_test)  # 부호 반전 (높을수록 이상치)\n",
    "\n",
    "# 성능 평가\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, ocsvm_pred, target_names=['정상', '사기']))\n",
    "\n",
    "ocsvm_auc = roc_auc_score(y_test, ocsvm_scores)\n",
    "ocsvm_f1 = f1_score(y_test, ocsvm_pred)\n",
    "ocsvm_ap = average_precision_score(y_test, ocsvm_scores)\n",
    "\n",
    "print(f\"ROC-AUC Score: {ocsvm_auc:.4f}\")\n",
    "print(f\"F1 Score: {ocsvm_f1:.4f}\")\n",
    "print(f\"Average Precision: {ocsvm_ap:.4f}\")\n",
    "\n",
    "results['One-Class SVM'] = {\n",
    "    'predictions': ocsvm_pred,\n",
    "    'probabilities': ocsvm_scores,\n",
    "    'roc_auc': ocsvm_auc,\n",
    "    'f1': ocsvm_f1,\n",
    "    'avg_precision': ocsvm_ap\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13663bb0",
   "metadata": {},
   "source": [
    "### 4.2 Autoencoder\n",
    "정상 데이터 패턴을 학습하고, 재구성 오류가 큰 샘플을 이상치로 탐지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c75bd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder (PyTorch - GPU 사용)\n",
    "print(\"=\" * 50)\n",
    "print(\"Autoencoder 학습 중... (PyTorch GPU)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 정상 데이터만 사용하여 학습\n",
    "X_train_normal = X_train[y_train == 0].values.astype(np.float32)\n",
    "X_test_values = X_test.values.astype(np.float32)\n",
    "\n",
    "# Autoencoder 모델 정의\n",
    "input_dim = X_train.shape[1]\n",
    "encoding_dim = 14\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # 인코더\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, encoding_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim * 2, encoding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # 디코더\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, encoding_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim * 2, input_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# 모델 생성 및 GPU로 이동\n",
    "autoencoder = Autoencoder(input_dim, encoding_dim).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "\n",
    "# 데이터 준비\n",
    "X_train_tensor = torch.FloatTensor(X_train_normal).to(device)\n",
    "train_dataset = TensorDataset(X_train_tensor, X_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "# 검증 데이터 분리\n",
    "val_size = int(0.1 * len(X_train_normal))\n",
    "train_data = X_train_normal[val_size:]\n",
    "val_data = X_train_normal[:val_size]\n",
    "\n",
    "train_tensor = torch.FloatTensor(train_data).to(device)\n",
    "val_tensor = torch.FloatTensor(val_data).to(device)\n",
    "train_dataset = TensorDataset(train_tensor, train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "# 학습\n",
    "epochs = 50\n",
    "patience = 5\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "print(f\"학습 디바이스: {device}\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    autoencoder.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_x, _ in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = autoencoder(batch_x)\n",
    "        loss = criterion(outputs, batch_x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    # 검증\n",
    "    autoencoder.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = autoencoder(val_tensor)\n",
    "        val_loss = criterion(val_outputs, val_tensor).item()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] - Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        best_model_state = autoencoder.state_dict().copy()\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# 최적 모델 복원\n",
    "if best_model_state:\n",
    "    autoencoder.load_state_dict(best_model_state)\n",
    "\n",
    "print(\"\\n학습 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477bf192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder 예측 및 평가\n",
    "autoencoder.eval()\n",
    "\n",
    "# 테스트 데이터 예측\n",
    "X_test_tensor = torch.FloatTensor(X_test_values).to(device)\n",
    "with torch.no_grad():\n",
    "    reconstructions = autoencoder(X_test_tensor).cpu().numpy()\n",
    "\n",
    "mse = np.mean(np.power(X_test_values - reconstructions, 2), axis=1)\n",
    "\n",
    "# 임곗값 설정 (정상 데이터 재구성 오류의 95 퍼센타일)\n",
    "X_train_normal_tensor = torch.FloatTensor(X_train_normal).to(device)\n",
    "with torch.no_grad():\n",
    "    train_reconstructions = autoencoder(X_train_normal_tensor).cpu().numpy()\n",
    "\n",
    "train_mse = np.mean(np.power(X_train_normal - train_reconstructions, 2), axis=1)\n",
    "threshold = np.percentile(train_mse, 95)\n",
    "\n",
    "print(f\"재구성 오류 임곗값 (95 퍼센타일): {threshold:.6f}\")\n",
    "\n",
    "# 임곗값 기반 예측\n",
    "ae_pred = (mse > threshold).astype(int)\n",
    "ae_scores = mse  # 재구성 오류를 점수로 사용\n",
    "\n",
    "# 성능 평가\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, ae_pred, target_names=['정상', '사기']))\n",
    "\n",
    "ae_auc = roc_auc_score(y_test, ae_scores)\n",
    "ae_f1 = f1_score(y_test, ae_pred)\n",
    "ae_ap = average_precision_score(y_test, ae_scores)\n",
    "\n",
    "print(f\"ROC-AUC Score: {ae_auc:.4f}\")\n",
    "print(f\"F1 Score: {ae_f1:.4f}\")\n",
    "print(f\"Average Precision: {ae_ap:.4f}\")\n",
    "\n",
    "results['Autoencoder'] = {\n",
    "    'predictions': ae_pred,\n",
    "    'probabilities': ae_scores,\n",
    "    'roc_auc': ae_auc,\n",
    "    'f1': ae_f1,\n",
    "    'avg_precision': ae_ap\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1887cea",
   "metadata": {},
   "source": [
    "### 4.3 Isolation Forest (추가 이상치 탐지 모델)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b9867a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolation Forest\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Isolation Forest 학습 중...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 예상 이상치 비율 설정\n",
    "contamination = y_train.sum() / len(y_train)\n",
    "\n",
    "iforest = IsolationForest(n_estimators=100, contamination=contamination, \n",
    "                          random_state=42, n_jobs=-1)\n",
    "iforest.fit(X_train)\n",
    "\n",
    "# 예측 (1: 정상, -1: 이상치)\n",
    "iforest_pred_raw = iforest.predict(X_test)\n",
    "iforest_pred = np.where(iforest_pred_raw == -1, 1, 0)\n",
    "\n",
    "# Anomaly score (낮을수록 이상치)\n",
    "iforest_scores = -iforest.decision_function(X_test)  # 부호 반전\n",
    "\n",
    "# 성능 평가\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, iforest_pred, target_names=['정상', '사기']))\n",
    "\n",
    "iforest_auc = roc_auc_score(y_test, iforest_scores)\n",
    "iforest_f1 = f1_score(y_test, iforest_pred)\n",
    "iforest_ap = average_precision_score(y_test, iforest_scores)\n",
    "\n",
    "print(f\"ROC-AUC Score: {iforest_auc:.4f}\")\n",
    "print(f\"F1 Score: {iforest_f1:.4f}\")\n",
    "print(f\"Average Precision: {iforest_ap:.4f}\")\n",
    "\n",
    "results['Isolation Forest'] = {\n",
    "    'predictions': iforest_pred,\n",
    "    'probabilities': iforest_scores,\n",
    "    'roc_auc': iforest_auc,\n",
    "    'f1': iforest_f1,\n",
    "    'avg_precision': iforest_ap\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6c4bcb",
   "metadata": {},
   "source": [
    "## 5. 모델 성능 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0ca760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성능 비교 요약 테이블\n",
    "print(\"=\" * 70)\n",
    "print(\"                    모델 성능 비교 요약\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "comparison_data = []\n",
    "for model_name, metrics in results.items():\n",
    "    comparison_data.append({\n",
    "        '모델': model_name,\n",
    "        'ROC-AUC': metrics['roc_auc'],\n",
    "        'F1 Score': metrics['f1'],\n",
    "        'Avg Precision': metrics['avg_precision']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('ROC-AUC', ascending=False)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# 모델 유형 분류\n",
    "comparison_df['유형'] = comparison_df['모델'].apply(\n",
    "    lambda x: '전통적 분류' if x in ['Logistic Regression', 'Random Forest'] else '이상치 탐지'\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"                    유형별 평균 성능\")\n",
    "print(\"=\" * 70)\n",
    "print(comparison_df.groupby('유형')[['ROC-AUC', 'F1 Score', 'Avg Precision']].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07c524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성능 지표 시각화\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "models = list(results.keys())\n",
    "colors = ['steelblue', 'green', 'orange', 'red', 'purple']\n",
    "\n",
    "# ROC-AUC 비교\n",
    "roc_aucs = [results[m]['roc_auc'] for m in models]\n",
    "axes[0].barh(models, roc_aucs, color=colors[:len(models)])\n",
    "axes[0].set_xlabel('ROC-AUC Score')\n",
    "axes[0].set_title('ROC-AUC 비교')\n",
    "axes[0].set_xlim([0.5, 1.0])\n",
    "for i, v in enumerate(roc_aucs):\n",
    "    axes[0].text(v + 0.01, i, f'{v:.4f}', va='center')\n",
    "\n",
    "# F1 Score 비교\n",
    "f1_scores = [results[m]['f1'] for m in models]\n",
    "axes[1].barh(models, f1_scores, color=colors[:len(models)])\n",
    "axes[1].set_xlabel('F1 Score')\n",
    "axes[1].set_title('F1 Score 비교')\n",
    "for i, v in enumerate(f1_scores):\n",
    "    axes[1].text(v + 0.01, i, f'{v:.4f}', va='center')\n",
    "\n",
    "# Average Precision 비교\n",
    "avg_precs = [results[m]['avg_precision'] for m in models]\n",
    "axes[2].barh(models, avg_precs, color=colors[:len(models)])\n",
    "axes[2].set_xlabel('Average Precision')\n",
    "axes[2].set_title('Average Precision 비교')\n",
    "for i, v in enumerate(avg_precs):\n",
    "    axes[2].text(v + 0.01, i, f'{v:.4f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da118ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve 비교\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ROC Curve\n",
    "for model_name, metrics in results.items():\n",
    "    fpr, tpr, _ = roc_curve(y_test, metrics['probabilities'])\n",
    "    axes[0].plot(fpr, tpr, label=f\"{model_name} (AUC={metrics['roc_auc']:.4f})\")\n",
    "\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].set_title('ROC Curve 비교')\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision-Recall Curve\n",
    "for model_name, metrics in results.items():\n",
    "    precision, recall, _ = precision_recall_curve(y_test, metrics['probabilities'])\n",
    "    axes[1].plot(recall, precision, label=f\"{model_name} (AP={metrics['avg_precision']:.4f})\")\n",
    "\n",
    "axes[1].set_xlabel('Recall')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].set_title('Precision-Recall Curve 비교')\n",
    "axes[1].legend(loc='upper right')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d063865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix 시각화\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (model_name, metrics) in enumerate(results.items()):\n",
    "    cm = confusion_matrix(y_test, metrics['predictions'])\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
    "                xticklabels=['정상', '사기'], yticklabels=['정상', '사기'])\n",
    "    axes[idx].set_title(f'{model_name}')\n",
    "    axes[idx].set_xlabel('예측')\n",
    "    axes[idx].set_ylabel('실제')\n",
    "\n",
    "# 빈 서브플롯 숨기기\n",
    "for idx in range(len(results), len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf74be6",
   "metadata": {},
   "source": [
    "## 6. 결론 및 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b9d448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 결론 출력\n",
    "print(\"=\" * 70)\n",
    "print(\"                         분석 결론\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 최고 성능 모델 찾기\n",
    "best_auc_model = max(results, key=lambda x: results[x]['roc_auc'])\n",
    "best_f1_model = max(results, key=lambda x: results[x]['f1'])\n",
    "best_ap_model = max(results, key=lambda x: results[x]['avg_precision'])\n",
    "\n",
    "print(f\"\\n최고 ROC-AUC 모델: {best_auc_model} ({results[best_auc_model]['roc_auc']:.4f})\")\n",
    "print(f\"최고 F1 Score 모델: {best_f1_model} ({results[best_f1_model]['f1']:.4f})\")\n",
    "print(f\"최고 Average Precision 모델: {best_ap_model} ({results[best_ap_model]['avg_precision']:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"분석 요약:\")\n",
    "print(\"-\" * 70)\n",
    "print(\"\"\"\n",
    "1. 전통적인 분류 모델 (Logistic Regression, Random Forest)\n",
    "   - 레이블 정보를 활용하여 지도 학습\n",
    "   - class_weight='balanced' 옵션으로 클래스 불균형 처리\n",
    "   - 일반적으로 높은 성능을 보임\n",
    "\n",
    "2. 이상치 탐지 모델 (One-Class SVM, Autoencoder, Isolation Forest)\n",
    "   - 정상 데이터의 패턴만 학습 (비지도/반지도 학습)\n",
    "   - 레이블이 없거나 부족한 상황에서 유용\n",
    "   - 새로운 유형의 사기 패턴 탐지에 강점\n",
    "\n",
    "3. 실무 적용 시 고려사항:\n",
    "   - 불균형 데이터: Average Precision이 더 신뢰할 수 있는 지표\n",
    "   - 실시간 탐지: Logistic Regression이 빠른 추론 속도 제공\n",
    "   - 해석 가능성: Random Forest는 특성 중요도 분석 가능\n",
    "   - 적응형 탐지: Autoencoder는 새로운 사기 패턴에 적응 가능\n",
    "\"\"\")\n",
    "\n",
    "# 모델별 장단점\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"                    모델별 특징 비교\")\n",
    "print(\"=\" * 70)\n",
    "model_features = pd.DataFrame({\n",
    "    '모델': ['Logistic Regression', 'Random Forest', 'One-Class SVM', 'Autoencoder', 'Isolation Forest'],\n",
    "    '학습 방식': ['지도학습', '지도학습', '비지도학습', '비지도학습', '비지도학습'],\n",
    "    '해석 가능성': ['높음', '중간', '낮음', '낮음', '중간'],\n",
    "    '학습 속도': ['빠름', '중간', '느림', '느림', '빠름'],\n",
    "    '추론 속도': ['빠름', '빠름', '중간', '중간', '빠름'],\n",
    "    '메모리 사용': ['낮음', '중간', '높음', '중간', '낮음']\n",
    "})\n",
    "print(model_features.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926f27f1",
   "metadata": {},
   "source": [
    "## 7. 추가 이상치 탐지 모델\n",
    "\n",
    "### 7.1 Local Outlier Factor (LOF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5417cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Outlier Factor (LOF)\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Local Outlier Factor (LOF) 학습 중...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# LOF는 novelty=True로 설정해야 새 데이터 예측 가능\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=contamination, novelty=True, n_jobs=-1)\n",
    "lof.fit(X_train)\n",
    "\n",
    "# 예측 (1: 정상, -1: 이상치)\n",
    "lof_pred_raw = lof.predict(X_test)\n",
    "lof_pred = np.where(lof_pred_raw == -1, 1, 0)\n",
    "\n",
    "# 이상치 점수 (낮을수록 이상치)\n",
    "lof_scores = -lof.decision_function(X_test)\n",
    "\n",
    "# 성능 평가\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, lof_pred, target_names=['정상', '사기']))\n",
    "\n",
    "lof_auc = roc_auc_score(y_test, lof_scores)\n",
    "lof_f1 = f1_score(y_test, lof_pred)\n",
    "lof_ap = average_precision_score(y_test, lof_scores)\n",
    "\n",
    "print(f\"ROC-AUC Score: {lof_auc:.4f}\")\n",
    "print(f\"F1 Score: {lof_f1:.4f}\")\n",
    "print(f\"Average Precision: {lof_ap:.4f}\")\n",
    "\n",
    "results['LOF'] = {\n",
    "    'predictions': lof_pred,\n",
    "    'probabilities': lof_scores,\n",
    "    'roc_auc': lof_auc,\n",
    "    'f1': lof_f1,\n",
    "    'avg_precision': lof_ap\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8823f64",
   "metadata": {},
   "source": [
    "### 7.2 DBSCAN (Density-Based Spatial Clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac47a67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"DBSCAN 학습 중...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 계산 효율을 위해 샘플링\n",
    "sample_size_dbscan = min(20000, len(X_train))\n",
    "X_train_sample_db = X_train.sample(n=sample_size_dbscan, random_state=42)\n",
    "\n",
    "# eps 파라미터 추정 (k-distance plot 기반)\n",
    "k = 5\n",
    "neigh = NearestNeighbors(n_neighbors=k)\n",
    "neigh.fit(X_train_sample_db)\n",
    "distances, _ = neigh.kneighbors(X_train_sample_db)\n",
    "k_distances = np.sort(distances[:, k-1])\n",
    "eps_value = np.percentile(k_distances, 95)  # 95 퍼센타일 사용\n",
    "\n",
    "print(f\"선택된 eps 값: {eps_value:.4f}\")\n",
    "\n",
    "# DBSCAN 학습\n",
    "dbscan = DBSCAN(eps=eps_value, min_samples=5, n_jobs=-1)\n",
    "dbscan_train_labels = dbscan.fit_predict(X_train_sample_db)\n",
    "\n",
    "# 테스트 데이터 예측을 위해 가장 가까운 학습 데이터 포인트 기반 예측\n",
    "# 노이즈(-1)로 분류된 학습 포인트와의 거리 기반\n",
    "noise_points = X_train_sample_db[dbscan_train_labels == -1]\n",
    "if len(noise_points) > 0:\n",
    "    neigh_noise = NearestNeighbors(n_neighbors=1)\n",
    "    neigh_noise.fit(noise_points)\n",
    "    distances_to_noise, _ = neigh_noise.kneighbors(X_test)\n",
    "    dbscan_scores = 1 / (distances_to_noise.flatten() + 1e-6)  # 가까울수록 높은 점수\n",
    "else:\n",
    "    # 노이즈 포인트가 없으면 전체 데이터와의 거리 사용\n",
    "    neigh_all = NearestNeighbors(n_neighbors=1)\n",
    "    neigh_all.fit(X_train_sample_db)\n",
    "    distances_all, _ = neigh_all.kneighbors(X_test)\n",
    "    dbscan_scores = distances_all.flatten()\n",
    "\n",
    "# 임곗값 기반 예측\n",
    "dbscan_threshold = np.percentile(dbscan_scores, 100 - contamination * 100)\n",
    "dbscan_pred = (dbscan_scores > dbscan_threshold).astype(int)\n",
    "\n",
    "# 성능 평가\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, dbscan_pred, target_names=['정상', '사기']))\n",
    "\n",
    "dbscan_auc = roc_auc_score(y_test, dbscan_scores)\n",
    "dbscan_f1 = f1_score(y_test, dbscan_pred)\n",
    "dbscan_ap = average_precision_score(y_test, dbscan_scores)\n",
    "\n",
    "print(f\"ROC-AUC Score: {dbscan_auc:.4f}\")\n",
    "print(f\"F1 Score: {dbscan_f1:.4f}\")\n",
    "print(f\"Average Precision: {dbscan_ap:.4f}\")\n",
    "\n",
    "results['DBSCAN'] = {\n",
    "    'predictions': dbscan_pred,\n",
    "    'probabilities': dbscan_scores,\n",
    "    'roc_auc': dbscan_auc,\n",
    "    'f1': dbscan_f1,\n",
    "    'avg_precision': dbscan_ap\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d41128",
   "metadata": {},
   "source": [
    "### 7.3 Variational Autoencoder (VAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c202b705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variational Autoencoder (VAE) - PyTorch GPU 버전\n",
    "print(\"=\" * 50)\n",
    "print(\"Variational Autoencoder (VAE) 학습 중... (PyTorch GPU)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# VAE 파라미터\n",
    "latent_dim_vae = 8\n",
    "intermediate_dim_vae = 32\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, intermediate_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # 인코더\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, intermediate_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(intermediate_dim, intermediate_dim // 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc_mu = nn.Linear(intermediate_dim // 2, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(intermediate_dim // 2, latent_dim)\n",
    "        \n",
    "        # 디코더\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, intermediate_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(intermediate_dim // 2, intermediate_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(intermediate_dim, input_dim)\n",
    "        )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "# 모델 생성 및 GPU로 이동\n",
    "vae = VAE(input_dim, latent_dim_vae, intermediate_dim_vae).to(device)\n",
    "vae_optimizer = optim.Adam(vae.parameters(), lr=0.001)\n",
    "\n",
    "# 데이터 준비\n",
    "X_train_normal_arr = X_train[y_train == 0].values.astype(np.float32)\n",
    "val_size_vae = int(0.1 * len(X_train_normal_arr))\n",
    "train_data_vae = X_train_normal_arr[val_size_vae:]\n",
    "val_data_vae = X_train_normal_arr[:val_size_vae]\n",
    "\n",
    "train_tensor_vae = torch.FloatTensor(train_data_vae).to(device)\n",
    "val_tensor_vae = torch.FloatTensor(val_data_vae).to(device)\n",
    "train_dataset_vae = TensorDataset(train_tensor_vae, train_tensor_vae)\n",
    "train_loader_vae = DataLoader(train_dataset_vae, batch_size=256, shuffle=True)\n",
    "\n",
    "# VAE 손실 함수\n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    recon_loss = nn.functional.mse_loss(recon_x, x, reduction='mean')\n",
    "    kl_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + 0.001 * kl_loss  # KL 가중치 조절\n",
    "\n",
    "# 학습\n",
    "epochs_vae = 50\n",
    "patience_vae = 5\n",
    "best_val_loss_vae = float('inf')\n",
    "patience_counter_vae = 0\n",
    "best_vae_state = None\n",
    "\n",
    "print(f\"학습 디바이스: {device}\")\n",
    "\n",
    "for epoch in range(epochs_vae):\n",
    "    vae.train()\n",
    "    train_loss_vae = 0.0\n",
    "    for batch_x, _ in train_loader_vae:\n",
    "        vae_optimizer.zero_grad()\n",
    "        recon, mu, logvar = vae(batch_x)\n",
    "        loss = vae_loss(recon, batch_x, mu, logvar)\n",
    "        loss.backward()\n",
    "        vae_optimizer.step()\n",
    "        train_loss_vae += loss.item()\n",
    "    \n",
    "    train_loss_vae /= len(train_loader_vae)\n",
    "    \n",
    "    # 검증\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        val_recon, val_mu, val_logvar = vae(val_tensor_vae)\n",
    "        val_loss_vae = vae_loss(val_recon, val_tensor_vae, val_mu, val_logvar).item()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs_vae}] - Train Loss: {train_loss_vae:.6f}, Val Loss: {val_loss_vae:.6f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss_vae < best_val_loss_vae:\n",
    "        best_val_loss_vae = val_loss_vae\n",
    "        patience_counter_vae = 0\n",
    "        best_vae_state = vae.state_dict().copy()\n",
    "    else:\n",
    "        patience_counter_vae += 1\n",
    "        if patience_counter_vae >= patience_vae:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# 최적 모델 복원\n",
    "if best_vae_state:\n",
    "    vae.load_state_dict(best_vae_state)\n",
    "\n",
    "print(\"\\nVAE 학습 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ac6bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE 예측 및 평가\n",
    "vae.eval()\n",
    "\n",
    "# 테스트 데이터 예측\n",
    "X_test_tensor_vae = torch.FloatTensor(X_test_values).to(device)\n",
    "with torch.no_grad():\n",
    "    vae_reconstructions, _, _ = vae(X_test_tensor_vae)\n",
    "    vae_reconstructions = vae_reconstructions.cpu().numpy()\n",
    "\n",
    "vae_mse = np.mean(np.power(X_test_values - vae_reconstructions, 2), axis=1)\n",
    "\n",
    "# 임곗값 설정\n",
    "X_train_normal_tensor_vae = torch.FloatTensor(X_train_normal_arr).to(device)\n",
    "with torch.no_grad():\n",
    "    vae_train_recon, _, _ = vae(X_train_normal_tensor_vae)\n",
    "    vae_train_recon = vae_train_recon.cpu().numpy()\n",
    "\n",
    "vae_train_mse = np.mean(np.power(X_train_normal_arr - vae_train_recon, 2), axis=1)\n",
    "vae_threshold = np.percentile(vae_train_mse, 95)\n",
    "\n",
    "print(f\"VAE 재구성 오류 임곗값 (95 퍼센타일): {vae_threshold:.6f}\")\n",
    "\n",
    "# 예측\n",
    "vae_pred = (vae_mse > vae_threshold).astype(int)\n",
    "vae_scores = vae_mse\n",
    "\n",
    "# 성능 평가\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, vae_pred, target_names=['정상', '사기']))\n",
    "\n",
    "vae_auc = roc_auc_score(y_test, vae_scores)\n",
    "vae_f1 = f1_score(y_test, vae_pred)\n",
    "vae_ap = average_precision_score(y_test, vae_scores)\n",
    "\n",
    "print(f\"ROC-AUC Score: {vae_auc:.4f}\")\n",
    "print(f\"F1 Score: {vae_f1:.4f}\")\n",
    "print(f\"Average Precision: {vae_ap:.4f}\")\n",
    "\n",
    "results['VAE'] = {\n",
    "    'predictions': vae_pred,\n",
    "    'probabilities': vae_scores,\n",
    "    'roc_auc': vae_auc,\n",
    "    'f1': vae_f1,\n",
    "    'avg_precision': vae_ap\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59253740",
   "metadata": {},
   "source": [
    "### 7.4 GAN (Generative Adversarial Network) 기반 이상치 탐지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aef7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN 기반 이상치 탐지 (PyTorch GPU 버전)\n",
    "print(\"=\" * 50)\n",
    "print(\"GAN 기반 이상치 탐지 학습 중... (PyTorch GPU)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# GAN 파라미터\n",
    "latent_dim_gan = 32\n",
    "\n",
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 모델 생성 및 GPU로 이동\n",
    "generator = Generator(latent_dim_gan, input_dim).to(device)\n",
    "discriminator = Discriminator(input_dim).to(device)\n",
    "\n",
    "# 옵티마이저\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "criterion_gan = nn.BCELoss()\n",
    "\n",
    "# 데이터 준비\n",
    "X_train_normal_gan = torch.FloatTensor(X_train_normal_arr).to(device)\n",
    "gan_dataset = TensorDataset(X_train_normal_gan)\n",
    "gan_loader = DataLoader(gan_dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "# 학습\n",
    "epochs_gan = 50\n",
    "print(f\"학습 디바이스: {device}\")\n",
    "\n",
    "for epoch in range(epochs_gan):\n",
    "    d_loss_epoch = 0.0\n",
    "    g_loss_epoch = 0.0\n",
    "    \n",
    "    for batch_idx, (real_samples,) in enumerate(gan_loader):\n",
    "        batch_size_curr = real_samples.size(0)\n",
    "        \n",
    "        # 레이블 생성\n",
    "        real_labels = torch.ones(batch_size_curr, 1).to(device) * 0.9  # Label smoothing\n",
    "        fake_labels = torch.zeros(batch_size_curr, 1).to(device)\n",
    "        \n",
    "        # -------- Discriminator 학습 --------\n",
    "        d_optimizer.zero_grad()\n",
    "        \n",
    "        # Real samples\n",
    "        real_output = discriminator(real_samples)\n",
    "        d_loss_real = criterion_gan(real_output, real_labels)\n",
    "        \n",
    "        # Fake samples\n",
    "        noise = torch.randn(batch_size_curr, latent_dim_gan).to(device)\n",
    "        fake_samples = generator(noise)\n",
    "        fake_output = discriminator(fake_samples.detach())\n",
    "        d_loss_fake = criterion_gan(fake_output, fake_labels)\n",
    "        \n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        # -------- Generator 학습 --------\n",
    "        g_optimizer.zero_grad()\n",
    "        \n",
    "        noise = torch.randn(batch_size_curr, latent_dim_gan).to(device)\n",
    "        fake_samples = generator(noise)\n",
    "        fake_output = discriminator(fake_samples)\n",
    "        g_loss = criterion_gan(fake_output, torch.ones(batch_size_curr, 1).to(device))\n",
    "        \n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "        d_loss_epoch += d_loss.item()\n",
    "        g_loss_epoch += g_loss.item()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs_gan}] - D Loss: {d_loss_epoch/len(gan_loader):.4f}, G Loss: {g_loss_epoch/len(gan_loader):.4f}\")\n",
    "\n",
    "print(\"\\nGAN 학습 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64965faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN 이상치 점수 계산 (Discriminator 기반)\n",
    "discriminator.eval()\n",
    "\n",
    "# 테스트 데이터에 대한 판별자 점수\n",
    "X_test_tensor_gan = torch.FloatTensor(X_test_values).to(device)\n",
    "with torch.no_grad():\n",
    "    gan_output = discriminator(X_test_tensor_gan).cpu().numpy().flatten()\n",
    "\n",
    "# 1에서 빼서 이상치일수록 높은 점수\n",
    "gan_scores = 1 - gan_output\n",
    "\n",
    "# 임곗값 설정\n",
    "with torch.no_grad():\n",
    "    gan_train_output = discriminator(X_train_normal_gan).cpu().numpy().flatten()\n",
    "\n",
    "gan_train_scores = 1 - gan_train_output\n",
    "gan_threshold = np.percentile(gan_train_scores, 95)\n",
    "\n",
    "print(f\"GAN 이상치 임곗값 (95 퍼센타일): {gan_threshold:.6f}\")\n",
    "\n",
    "# 예측\n",
    "gan_pred = (gan_scores > gan_threshold).astype(int)\n",
    "\n",
    "# 성능 평가\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, gan_pred, target_names=['정상', '사기']))\n",
    "\n",
    "gan_auc = roc_auc_score(y_test, gan_scores)\n",
    "gan_f1 = f1_score(y_test, gan_pred)\n",
    "gan_ap = average_precision_score(y_test, gan_scores)\n",
    "\n",
    "print(f\"ROC-AUC Score: {gan_auc:.4f}\")\n",
    "print(f\"F1 Score: {gan_f1:.4f}\")\n",
    "print(f\"Average Precision: {gan_ap:.4f}\")\n",
    "\n",
    "results['GAN'] = {\n",
    "    'predictions': gan_pred,\n",
    "    'probabilities': gan_scores,\n",
    "    'roc_auc': gan_auc,\n",
    "    'f1': gan_f1,\n",
    "    'avg_precision': gan_ap\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb80667a",
   "metadata": {},
   "source": [
    "### 7.5 LSTM Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2083aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Autoencoder (PyTorch GPU 버전)\n",
    "print(\"=\" * 50)\n",
    "print(\"LSTM Autoencoder 학습 중... (PyTorch GPU)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "timesteps = 10\n",
    "n_features = input_dim // timesteps + (1 if input_dim % timesteps != 0 else 0)\n",
    "\n",
    "# 패딩을 위한 크기 조정\n",
    "padded_dim = timesteps * n_features\n",
    "X_train_normal_padded = np.zeros((len(X_train_normal_arr), padded_dim), dtype=np.float32)\n",
    "X_train_normal_padded[:, :input_dim] = X_train_normal_arr\n",
    "X_train_lstm = X_train_normal_padded.reshape(-1, timesteps, n_features)\n",
    "\n",
    "X_test_padded = np.zeros((len(X_test_values), padded_dim), dtype=np.float32)\n",
    "X_test_padded[:, :input_dim] = X_test_values\n",
    "X_test_lstm = X_test_padded.reshape(-1, timesteps, n_features)\n",
    "\n",
    "print(f\"LSTM 입력 shape: {X_train_lstm.shape}\")\n",
    "\n",
    "# LSTM Autoencoder 모델 정의\n",
    "class LSTMAutoencoder(nn.Module):\n",
    "    def __init__(self, n_features, hidden_dim=32, latent_dim=16, timesteps=10):\n",
    "        super(LSTMAutoencoder, self).__init__()\n",
    "        self.timesteps = timesteps\n",
    "        self.n_features = n_features\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # 인코더 LSTM\n",
    "        self.encoder_lstm1 = nn.LSTM(n_features, hidden_dim, batch_first=True)\n",
    "        self.encoder_lstm2 = nn.LSTM(hidden_dim, latent_dim, batch_first=True)\n",
    "        \n",
    "        # 디코더 LSTM\n",
    "        self.decoder_lstm1 = nn.LSTM(latent_dim, latent_dim, batch_first=True)\n",
    "        self.decoder_lstm2 = nn.LSTM(latent_dim, hidden_dim, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_dim, n_features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 인코더\n",
    "        enc_out1, _ = self.encoder_lstm1(x)\n",
    "        enc_out2, (h_n, c_n) = self.encoder_lstm2(enc_out1)\n",
    "        \n",
    "        # 잠재 벡터를 시퀀스로 반복\n",
    "        latent = h_n.squeeze(0).unsqueeze(1).repeat(1, self.timesteps, 1)\n",
    "        \n",
    "        # 디코더\n",
    "        dec_out1, _ = self.decoder_lstm1(latent)\n",
    "        dec_out2, _ = self.decoder_lstm2(dec_out1)\n",
    "        output = self.output_layer(dec_out2)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# 모델 생성 및 GPU로 이동\n",
    "lstm_autoencoder = LSTMAutoencoder(n_features, hidden_dim=32, latent_dim=16, timesteps=timesteps).to(device)\n",
    "lstm_optimizer = optim.Adam(lstm_autoencoder.parameters(), lr=0.001)\n",
    "lstm_criterion = nn.MSELoss()\n",
    "\n",
    "# 데이터 준비\n",
    "val_size_lstm = int(0.1 * len(X_train_lstm))\n",
    "train_data_lstm = X_train_lstm[val_size_lstm:]\n",
    "val_data_lstm = X_train_lstm[:val_size_lstm]\n",
    "\n",
    "train_tensor_lstm = torch.FloatTensor(train_data_lstm).to(device)\n",
    "val_tensor_lstm = torch.FloatTensor(val_data_lstm).to(device)\n",
    "train_dataset_lstm = TensorDataset(train_tensor_lstm, train_tensor_lstm)\n",
    "train_loader_lstm = DataLoader(train_dataset_lstm, batch_size=256, shuffle=True)\n",
    "\n",
    "# 학습\n",
    "epochs_lstm = 50\n",
    "patience_lstm = 5\n",
    "best_val_loss_lstm = float('inf')\n",
    "patience_counter_lstm = 0\n",
    "best_lstm_state = None\n",
    "\n",
    "print(f\"학습 디바이스: {device}\")\n",
    "\n",
    "for epoch in range(epochs_lstm):\n",
    "    lstm_autoencoder.train()\n",
    "    train_loss_lstm = 0.0\n",
    "    for batch_x, _ in train_loader_lstm:\n",
    "        lstm_optimizer.zero_grad()\n",
    "        outputs = lstm_autoencoder(batch_x)\n",
    "        loss = lstm_criterion(outputs, batch_x)\n",
    "        loss.backward()\n",
    "        lstm_optimizer.step()\n",
    "        train_loss_lstm += loss.item()\n",
    "    \n",
    "    train_loss_lstm /= len(train_loader_lstm)\n",
    "    \n",
    "    # 검증\n",
    "    lstm_autoencoder.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs_lstm = lstm_autoencoder(val_tensor_lstm)\n",
    "        val_loss_lstm = lstm_criterion(val_outputs_lstm, val_tensor_lstm).item()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs_lstm}] - Train Loss: {train_loss_lstm:.6f}, Val Loss: {val_loss_lstm:.6f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss_lstm < best_val_loss_lstm:\n",
    "        best_val_loss_lstm = val_loss_lstm\n",
    "        patience_counter_lstm = 0\n",
    "        best_lstm_state = lstm_autoencoder.state_dict().copy()\n",
    "    else:\n",
    "        patience_counter_lstm += 1\n",
    "        if patience_counter_lstm >= patience_lstm:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# 최적 모델 복원\n",
    "if best_lstm_state:\n",
    "    lstm_autoencoder.load_state_dict(best_lstm_state)\n",
    "\n",
    "print(\"\\nLSTM Autoencoder 학습 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1524aa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Autoencoder 예측 및 평가\n",
    "lstm_autoencoder.eval()\n",
    "\n",
    "# 테스트 데이터 예측\n",
    "X_test_lstm_tensor = torch.FloatTensor(X_test_lstm).to(device)\n",
    "with torch.no_grad():\n",
    "    lstm_reconstructions = lstm_autoencoder(X_test_lstm_tensor).cpu().numpy()\n",
    "\n",
    "lstm_mse = np.mean(np.power(X_test_lstm - lstm_reconstructions, 2), axis=(1, 2))\n",
    "\n",
    "# 임곗값 설정\n",
    "X_train_lstm_tensor = torch.FloatTensor(X_train_lstm).to(device)\n",
    "with torch.no_grad():\n",
    "    lstm_train_reconstructions = lstm_autoencoder(X_train_lstm_tensor).cpu().numpy()\n",
    "\n",
    "lstm_train_mse = np.mean(np.power(X_train_lstm - lstm_train_reconstructions, 2), axis=(1, 2))\n",
    "lstm_threshold = np.percentile(lstm_train_mse, 95)\n",
    "\n",
    "print(f\"LSTM 재구성 오류 임곗값 (95 퍼센타일): {lstm_threshold:.6f}\")\n",
    "\n",
    "# 예측\n",
    "lstm_pred = (lstm_mse > lstm_threshold).astype(int)\n",
    "lstm_scores = lstm_mse\n",
    "\n",
    "# 성능 평가\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, lstm_pred, target_names=['정상', '사기']))\n",
    "\n",
    "lstm_auc = roc_auc_score(y_test, lstm_scores)\n",
    "lstm_f1 = f1_score(y_test, lstm_pred)\n",
    "lstm_ap = average_precision_score(y_test, lstm_scores)\n",
    "\n",
    "print(f\"ROC-AUC Score: {lstm_auc:.4f}\")\n",
    "print(f\"F1 Score: {lstm_f1:.4f}\")\n",
    "print(f\"Average Precision: {lstm_ap:.4f}\")\n",
    "\n",
    "results['LSTM'] = {\n",
    "    'predictions': lstm_pred,\n",
    "    'probabilities': lstm_scores,\n",
    "    'roc_auc': lstm_auc,\n",
    "    'f1': lstm_f1,\n",
    "    'avg_precision': lstm_ap\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246c564c",
   "metadata": {},
   "source": [
    "### 7.6 Simple RNN Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c5f349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple RNN Autoencoder (PyTorch GPU 버전)\n",
    "print(\"=\" * 50)\n",
    "print(\"Simple RNN Autoencoder 학습 중... (PyTorch GPU)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# RNN Autoencoder 모델 정의\n",
    "class RNNAutoencoder(nn.Module):\n",
    "    def __init__(self, n_features, hidden_dim=32, latent_dim=16, timesteps=10):\n",
    "        super(RNNAutoencoder, self).__init__()\n",
    "        self.timesteps = timesteps\n",
    "        self.n_features = n_features\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # 인코더 RNN\n",
    "        self.encoder_rnn1 = nn.RNN(n_features, hidden_dim, batch_first=True, nonlinearity='tanh')\n",
    "        self.encoder_rnn2 = nn.RNN(hidden_dim, latent_dim, batch_first=True, nonlinearity='tanh')\n",
    "        \n",
    "        # 디코더 RNN\n",
    "        self.decoder_rnn1 = nn.RNN(latent_dim, latent_dim, batch_first=True, nonlinearity='tanh')\n",
    "        self.decoder_rnn2 = nn.RNN(latent_dim, hidden_dim, batch_first=True, nonlinearity='tanh')\n",
    "        self.output_layer = nn.Linear(hidden_dim, n_features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 인코더\n",
    "        enc_out1, _ = self.encoder_rnn1(x)\n",
    "        enc_out2, h_n = self.encoder_rnn2(enc_out1)\n",
    "        \n",
    "        # 잠재 벡터를 시퀀스로 반복\n",
    "        latent = h_n.squeeze(0).unsqueeze(1).repeat(1, self.timesteps, 1)\n",
    "        \n",
    "        # 디코더\n",
    "        dec_out1, _ = self.decoder_rnn1(latent)\n",
    "        dec_out2, _ = self.decoder_rnn2(dec_out1)\n",
    "        output = self.output_layer(dec_out2)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# 모델 생성 및 GPU로 이동\n",
    "rnn_autoencoder = RNNAutoencoder(n_features, hidden_dim=32, latent_dim=16, timesteps=timesteps).to(device)\n",
    "rnn_optimizer = optim.Adam(rnn_autoencoder.parameters(), lr=0.001)\n",
    "rnn_criterion = nn.MSELoss()\n",
    "\n",
    "# 데이터 로더 (LSTM과 동일한 데이터 사용)\n",
    "train_dataset_rnn = TensorDataset(train_tensor_lstm, train_tensor_lstm)\n",
    "train_loader_rnn = DataLoader(train_dataset_rnn, batch_size=256, shuffle=True)\n",
    "\n",
    "# 학습\n",
    "epochs_rnn = 50\n",
    "patience_rnn = 5\n",
    "best_val_loss_rnn = float('inf')\n",
    "patience_counter_rnn = 0\n",
    "best_rnn_state = None\n",
    "\n",
    "print(f\"학습 디바이스: {device}\")\n",
    "\n",
    "for epoch in range(epochs_rnn):\n",
    "    rnn_autoencoder.train()\n",
    "    train_loss_rnn = 0.0\n",
    "    for batch_x, _ in train_loader_rnn:\n",
    "        rnn_optimizer.zero_grad()\n",
    "        outputs = rnn_autoencoder(batch_x)\n",
    "        loss = rnn_criterion(outputs, batch_x)\n",
    "        loss.backward()\n",
    "        rnn_optimizer.step()\n",
    "        train_loss_rnn += loss.item()\n",
    "    \n",
    "    train_loss_rnn /= len(train_loader_rnn)\n",
    "    \n",
    "    # 검증\n",
    "    rnn_autoencoder.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs_rnn = rnn_autoencoder(val_tensor_lstm)\n",
    "        val_loss_rnn = rnn_criterion(val_outputs_rnn, val_tensor_lstm).item()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs_rnn}] - Train Loss: {train_loss_rnn:.6f}, Val Loss: {val_loss_rnn:.6f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss_rnn < best_val_loss_rnn:\n",
    "        best_val_loss_rnn = val_loss_rnn\n",
    "        patience_counter_rnn = 0\n",
    "        best_rnn_state = rnn_autoencoder.state_dict().copy()\n",
    "    else:\n",
    "        patience_counter_rnn += 1\n",
    "        if patience_counter_rnn >= patience_rnn:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# 최적 모델 복원\n",
    "if best_rnn_state:\n",
    "    rnn_autoencoder.load_state_dict(best_rnn_state)\n",
    "\n",
    "print(\"\\nSimple RNN Autoencoder 학습 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6f47b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN Autoencoder 예측 및 평가\n",
    "rnn_autoencoder.eval()\n",
    "\n",
    "# 테스트 데이터 예측\n",
    "with torch.no_grad():\n",
    "    rnn_reconstructions = rnn_autoencoder(X_test_lstm_tensor).cpu().numpy()\n",
    "\n",
    "rnn_mse = np.mean(np.power(X_test_lstm - rnn_reconstructions, 2), axis=(1, 2))\n",
    "\n",
    "# 임곗값 설정\n",
    "with torch.no_grad():\n",
    "    rnn_train_reconstructions = rnn_autoencoder(X_train_lstm_tensor).cpu().numpy()\n",
    "\n",
    "rnn_train_mse = np.mean(np.power(X_train_lstm - rnn_train_reconstructions, 2), axis=(1, 2))\n",
    "rnn_threshold = np.percentile(rnn_train_mse, 95)\n",
    "\n",
    "print(f\"RNN 재구성 오류 임곗값 (95 퍼센타일): {rnn_threshold:.6f}\")\n",
    "\n",
    "# 예측\n",
    "rnn_pred = (rnn_mse > rnn_threshold).astype(int)\n",
    "rnn_scores = rnn_mse\n",
    "\n",
    "# 성능 평가\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, rnn_pred, target_names=['정상', '사기']))\n",
    "\n",
    "rnn_auc = roc_auc_score(y_test, rnn_scores)\n",
    "rnn_f1 = f1_score(y_test, rnn_pred)\n",
    "rnn_ap = average_precision_score(y_test, rnn_scores)\n",
    "\n",
    "print(f\"ROC-AUC Score: {rnn_auc:.4f}\")\n",
    "print(f\"F1 Score: {rnn_f1:.4f}\")\n",
    "print(f\"Average Precision: {rnn_ap:.4f}\")\n",
    "\n",
    "results['RNN'] = {\n",
    "    'predictions': rnn_pred,\n",
    "    'probabilities': rnn_scores,\n",
    "    'roc_auc': rnn_auc,\n",
    "    'f1': rnn_f1,\n",
    "    'avg_precision': rnn_ap\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49541a72",
   "metadata": {},
   "source": [
    "## 8. 전체 모델 성능 비교 (업데이트)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925bf0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 모델 성능 비교 요약\n",
    "print(\"=\" * 80)\n",
    "print(\"                        전체 모델 성능 비교 요약\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 결과 데이터프레임 생성\n",
    "comparison_data_all = []\n",
    "for model_name, metrics in results.items():\n",
    "    comparison_data_all.append({\n",
    "        '모델': model_name,\n",
    "        'ROC-AUC': round(metrics['roc_auc'], 4),\n",
    "        'F1 Score': round(metrics['f1'], 4),\n",
    "        'Avg Precision': round(metrics['avg_precision'], 4)\n",
    "    })\n",
    "\n",
    "comparison_df_all = pd.DataFrame(comparison_data_all)\n",
    "\n",
    "# 모델 유형 분류\n",
    "def get_model_type(model):\n",
    "    if model in ['Logistic Regression', 'Random Forest']:\n",
    "        return '전통적 분류'\n",
    "    elif model in ['One-Class SVM', 'Isolation Forest', 'LOF', 'DBSCAN']:\n",
    "        return '전통적 이상치 탐지'\n",
    "    else:\n",
    "        return '딥러닝 기반'\n",
    "\n",
    "comparison_df_all['유형'] = comparison_df_all['모델'].apply(get_model_type)\n",
    "\n",
    "# ROC-AUC 기준 정렬\n",
    "comparison_df_all = comparison_df_all.sort_values('ROC-AUC', ascending=False)\n",
    "print(comparison_df_all.to_string(index=False))\n",
    "\n",
    "# 최고 성능 모델\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"                            최고 성능 모델\")\n",
    "print(\"=\" * 80)\n",
    "best_auc = comparison_df_all.loc[comparison_df_all['ROC-AUC'].idxmax()]\n",
    "best_f1 = comparison_df_all.loc[comparison_df_all['F1 Score'].idxmax()]\n",
    "best_ap = comparison_df_all.loc[comparison_df_all['Avg Precision'].idxmax()]\n",
    "\n",
    "print(f\"📊 최고 ROC-AUC: {best_auc['모델']} ({best_auc['ROC-AUC']})\")\n",
    "print(f\"📊 최고 F1 Score: {best_f1['모델']} ({best_f1['F1 Score']})\")\n",
    "print(f\"📊 최고 Average Precision: {best_ap['모델']} ({best_ap['Avg Precision']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38a94eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유형별 평균 성능\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"                          유형별 평균 성능\")\n",
    "print(\"=\" * 80)\n",
    "type_avg = comparison_df_all.groupby('유형')[['ROC-AUC', 'F1 Score', 'Avg Precision']].mean()\n",
    "print(type_avg.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f38bfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 모델 시각화\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 8))\n",
    "\n",
    "# 색상 설정\n",
    "color_map = {\n",
    "    '전통적 분류': 'steelblue',\n",
    "    '전통적 이상치 탐지': 'forestgreen',\n",
    "    '딥러닝 기반': 'coral'\n",
    "}\n",
    "colors_all = [color_map[t] for t in comparison_df_all['유형']]\n",
    "\n",
    "models_all = comparison_df_all['모델'].tolist()\n",
    "\n",
    "# ROC-AUC\n",
    "ax1 = axes[0]\n",
    "bars1 = ax1.barh(models_all, comparison_df_all['ROC-AUC'].tolist(), color=colors_all)\n",
    "ax1.set_xlabel('ROC-AUC Score')\n",
    "ax1.set_title('ROC-AUC 비교')\n",
    "ax1.set_xlim([0.4, 1.0])\n",
    "for i, v in enumerate(comparison_df_all['ROC-AUC'].tolist()):\n",
    "    ax1.text(v + 0.01, i, f'{v:.4f}', va='center', fontsize=8)\n",
    "\n",
    "# F1 Score\n",
    "ax2 = axes[1]\n",
    "bars2 = ax2.barh(models_all, comparison_df_all['F1 Score'].tolist(), color=colors_all)\n",
    "ax2.set_xlabel('F1 Score')\n",
    "ax2.set_title('F1 Score 비교')\n",
    "for i, v in enumerate(comparison_df_all['F1 Score'].tolist()):\n",
    "    ax2.text(v + 0.01, i, f'{v:.4f}', va='center', fontsize=8)\n",
    "\n",
    "# Average Precision\n",
    "ax3 = axes[2]\n",
    "bars3 = ax3.barh(models_all, comparison_df_all['Avg Precision'].tolist(), color=colors_all)\n",
    "ax3.set_xlabel('Average Precision')\n",
    "ax3.set_title('Average Precision 비교')\n",
    "for i, v in enumerate(comparison_df_all['Avg Precision'].tolist()):\n",
    "    ax3.text(v + 0.01, i, f'{v:.4f}', va='center', fontsize=8)\n",
    "\n",
    "# 범례\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='steelblue', label='전통적 분류'),\n",
    "    Patch(facecolor='forestgreen', label='전통적 이상치 탐지'),\n",
    "    Patch(facecolor='coral', label='딥러닝 기반')\n",
    "]\n",
    "fig.legend(handles=legend_elements, loc='upper center', ncol=3, bbox_to_anchor=(0.5, 1.02))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.92)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf5701a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve - 전체 모델 비교\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# ROC Curve\n",
    "ax1 = axes[0]\n",
    "for model_name, metrics in results.items():\n",
    "    fpr, tpr, _ = roc_curve(y_test, metrics['probabilities'])\n",
    "    ax1.plot(fpr, tpr, label=f\"{model_name} ({metrics['roc_auc']:.3f})\", linewidth=1.5)\n",
    "\n",
    "ax1.plot([0, 1], [0, 1], 'k--', label='Random', linewidth=1)\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "ax1.set_title('ROC Curve - 전체 모델 비교')\n",
    "ax1.legend(loc='lower right', fontsize=8)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Precision-Recall Curve\n",
    "ax2 = axes[1]\n",
    "for model_name, metrics in results.items():\n",
    "    precision, recall, _ = precision_recall_curve(y_test, metrics['probabilities'])\n",
    "    ax2.plot(recall, precision, label=f\"{model_name} ({metrics['avg_precision']:.3f})\", linewidth=1.5)\n",
    "\n",
    "ax2.set_xlabel('Recall')\n",
    "ax2.set_ylabel('Precision')\n",
    "ax2.set_title('Precision-Recall Curve - 전체 모델 비교')\n",
    "ax2.legend(loc='upper right', fontsize=8)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93a2d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix - 전체 모델\n",
    "n_models = len(results)\n",
    "n_cols = 4\n",
    "n_rows = (n_models + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 4 * n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (model_name, metrics) in enumerate(results.items()):\n",
    "    cm = confusion_matrix(y_test, metrics['predictions'])\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
    "                xticklabels=['정상', '사기'], yticklabels=['정상', '사기'])\n",
    "    axes[idx].set_title(f'{model_name}', fontsize=10)\n",
    "    axes[idx].set_xlabel('예측')\n",
    "    axes[idx].set_ylabel('실제')\n",
    "\n",
    "# 빈 서브플롯 숨기기\n",
    "for idx in range(len(results), len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271eb1de",
   "metadata": {},
   "source": [
    "## 9. 최종 결론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72a6179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 결론 및 분석\n",
    "print(\"=\" * 80)\n",
    "print(\"                           최종 분석 결론\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "신용카드 사기 탐지 모델 성능 비교 분석\n",
    "\n",
    "1. 전통적 분류 모델 (Logistic Regression, Random Forest)\n",
    "   - 레이블 정보를 활용한 지도 학습 방식\n",
    "   - 클래스 불균형 처리(class_weight='balanced')로 좋은 성능\n",
    "   - Random Forest가 전반적으로 가장 우수한 성능\n",
    "\n",
    "2. 전통적 이상치 탐지 모델 (One-Class SVM, LOF, Isolation Forest, DBSCAN)\n",
    "   - 비지도/반지도 학습 방식으로 레이블 없이도 학습 가능\n",
    "   - Isolation Forest가 이상치 탐지 모델 중 가장 안정적\n",
    "   - LOF는 밀도 기반으로 지역적 이상치 탐지에 강점\n",
    "   - DBSCAN은 클러스터링 기반으로 파라미터 민감도가 높음\n",
    "\n",
    "3. 딥러닝 기반 모델 (Autoencoder, VAE, GAN, LSTM, RNN)\n",
    "   - 정상 데이터 패턴 학습 후 재구성 오류로 이상치 판별\n",
    "   - Autoencoder/VAE가 비교적 안정적인 성능\n",
    "   - LSTM/RNN은 시계열적 패턴이 명확할 때 효과적\n",
    "   - GAN은 학습이 불안정할 수 있으나 새로운 패턴 탐지에 유용\n",
    "\n",
    "4. 실무 적용 권장사항:\n",
    "   - 레이블 있는 경우: Random Forest 또는 Logistic Regression\n",
    "   - 레이블 없는 경우: Isolation Forest 또는 Autoencoder\n",
    "   - 실시간 처리: Logistic Regression (빠른 추론)\n",
    "   - 해석 필요: Random Forest (특성 중요도)\n",
    "   - 새로운 패턴: VAE 또는 GAN 기반 모델\n",
    "\"\"\")\n",
    "\n",
    "# 모델별 특징 비교표\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"                        모델별 특징 비교\")\n",
    "print(\"=\" * 80)\n",
    "model_features_all = pd.DataFrame({\n",
    "    '모델': list(results.keys()),\n",
    "    '유형': [get_model_type(m) for m in results.keys()],\n",
    "    'ROC-AUC': [results[m]['roc_auc'] for m in results.keys()],\n",
    "    'F1': [results[m]['f1'] for m in results.keys()],\n",
    "    'AP': [results[m]['avg_precision'] for m in results.keys()]\n",
    "})\n",
    "model_features_all = model_features_all.sort_values('ROC-AUC', ascending=False)\n",
    "print(model_features_all.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.12.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
